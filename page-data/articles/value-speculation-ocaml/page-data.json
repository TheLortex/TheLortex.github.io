{"componentChunkName":"component---src-pages-mdx-frontmatter-variant-mdx-frontmatter-slug-tsx-content-file-path-src-data-articles-2023-05-05-value-speculation-ocaml-mdx","path":"/articles/value-speculation-ocaml/","result":{"data":{"mdx":{"frontmatter":{"title":"Implementing value speculation in OCaml","description":"Value speculation exploits the CPU branch predictor to improve instruction parallelism. Here is an example of it in OCaml.","date":"2023-05-05","tags":null,"inline":null,"slug":"value-speculation-ocaml","hero_image_alt":"This is convoluted","variant":"articles","hero_image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#9898a8","images":{"fallback":{"src":"/static/8acd8434e4a2fc442e65896b7b7c1894/ae980/seum_hero.png","srcSet":"/static/8acd8434e4a2fc442e65896b7b7c1894/b216d/seum_hero.png 161w,\n/static/8acd8434e4a2fc442e65896b7b7c1894/6ef0d/seum_hero.png 321w,\n/static/8acd8434e4a2fc442e65896b7b7c1894/ae980/seum_hero.png 642w","sizes":"(min-width: 642px) 642px, 100vw"},"sources":[{"srcSet":"/static/8acd8434e4a2fc442e65896b7b7c1894/d272d/seum_hero.webp 161w,\n/static/8acd8434e4a2fc442e65896b7b7c1894/e567c/seum_hero.webp 321w,\n/static/8acd8434e4a2fc442e65896b7b7c1894/81d96/seum_hero.webp 642w","type":"image/webp","sizes":"(min-width: 642px) 642px, 100vw"}]},"width":642,"height":475.99999999999994}}}},"body":"\nCPUs are very good at doing things in parallel, even in a single core context.\nIndeed, speculative execution of code and instruction reordering helps the CPU\nensure that the pipeline is always full. However, data dependencies in the\nsequence of instructions might cause the CPU to have to wait for data, be it\nfrom the L1 cache or the much slower RAM storage. Francesco Mazzoli shows in their blog post, [Beating the L1 cache with value speculation](https://mazzo.li/posts/value-speculation.html) ,\nthat optimizing the critical path will in some conditions yield huge performance\nimprovements.\n\nThis article demonstrates that this optimisation can be implemented in the\nOCaml programming language. Knowing how OCaml values are represented in memory is useful, here is a chapter of Real World OCaml on that matter: [Memory Representation of Values](https://dev.realworldocaml.org/runtime-memory-layout.html).\n\n## Fast list iterations using one simple trick™\n\nLet's start by instantiating a linked list of 10000 random numbers.\n\n```ocaml\nlet l = List.init 10000 (fun _ -> Random.int 1024)\n```\n\nNow, let's sum the numbers 100k times and see how long that takes. To obtain these statistics, `perf stat` was used with the default settings on programs compiled with OCaml 5.0.\n\n```ocaml\nlet rec sum (accumulator: int) (cell: int list) =\n  match cell with\n  | head::tail -> sum (accumulator + head) tail\n  | [] -> accumulator\n\nfor _ = 1 to 100000 do\n  ignore (sum 0 l)\ndone\n```\n\n```\n Performance counter stats for './a.out sum':\n\n          1 368,91 msec task-clock:u                     #    0,999 CPUs utilized\n     4 835 597 233      cycles:u                         #    3,532 GHz\n     9 005 641 989      instructions:u                   #    1,86  insn per cycle\n     3 001 229 709      branches:u                       #    2,192 G/sec\n           195 819      branch-misses:u                  #    0,01% of all branches\n```\n\nConveniently, we're doing one billion additions (10000 list items, repeated 100k times). So each iteration is taking:\n\n- 1.36 nanoseconds\n- 4.8 cycles\n- 9 instructions\n- 3 branches\n\nWe can already see that the CPU is cramming multiple instructions per cycle.\nUsing the [compiler explorer](https://godbolt.org/), the following assembly is obtained:\n\n```nasm\ncamlExample__sum_268:\n        subq    $8, %rsp\n.L101:\n        cmpq    (%r14), %r15         # check if GC is waiting for us\n        jbe     .L102                # branch #1\n.L103:\n        testb   $1, %bl              # check if at end of list\n        je      .L100                # branch #2\n        addq    $8, %rsp\n        ret\n.L100:\n        movq    8(%rbx), %rdi        # load tail element of list\n        movq    (%rbx), %rbx         # load head element of list\n        leaq    -1(%rbx,%rax), %rax  # add it to accumulator\n        movq    %rdi, %rbx           # move tail element for next iteration\n        jmp     .L101\n.L102:\n        call    caml_call_gc@PLT\n.L104:\n        jmp     .L103\n```\n\nI've tried to think like a CPU in order to explain the numbers according to the perf results, but as so many pieces are involved I assumed it would be hard for everything to be correct. Instead, we'll try to get a good _intuition_ by thinking in terms of _data dependency_. Basically, the assumption is that things that do not depend on each other can be ran in parallel. The second assumption is that thanks to the _branch predictor_, branch instructions are supposed to have zero cost and they don't introduce data dependencies. Instead, the CPU predicts whether the program will go through the branch and continue execution. In case of misprediction, the CPU will roll back computations for us.\n\nSo here is the data dependency chart for this program, showing how two iterations are expected to be ran, with the critical path in red:\n\n![data chart](./sum_chart.png)\n\nEven if the tail pointer is loaded from cache, we still have to pay a 4 cycles cost to fetch from the L1 cache.\n\nThe **Check GC** part of the loop is independent from the rest, and is useful in a multicore context as every domain of the program has to synchronize when performing garbage collection.\n\n## Value speculation\n\nIt's time to open the rune book and perform some dark magic. We know that we\nwon't get past that 4 cycles per iteration bottleneck unless there is a way to\nbypass the pointer chasing. To do that, we're going to do what was done in\nthe value speculation article, but in pure OCaml.\n\nimport ObjMagic from \"./obj_magic.jpg\";\n\nThe principle is the same: `cell` is converted to a \"pointer\" using the forbidden <a href={ObjMagic}>`Obj.magic`</a> function. Using the value of the `previous` list\ncell pointer, we can compute `delta`, the number of bytes between the two last cells.\nThis is used to estimate where will be the next cell. If the prediction is correct,\nthe memory load of the next cell address is effectively bypassed, and the CPU can\ndirectly start working on the next iteration. If not, the branch predictor rolls back the\ncomputation.{\" \"}\n\n```ocaml\nlet current : int = Obj.magic cell in                   (* convert list cell to pointer (integer value) *)\nlet delta : int = current - previous                    (* compute delta with previous pointer *)\nlet prediction : int list = Obj.magic (current + delta) (* use delta to predict next pointer*)\n```\n\n![illustration of the delta computing mechanism ](./seum_delta_1.png)\n\nHere is the full program. Note that the delta calculation was inlined for performance reasons.\n\n(`current + delta = current + (current - previous) = 2 * current - previous`)\n\n```ocaml\nlet rec seum (previous : int) (accu : int) (cell : int list) =\n  match cell with\n  | [] -> accu\n  | head::tail ->\n    let current : int = Obj.magic cell in\n    let prediction : int list = (* compute prediction *)\n      Obj.magic (2 * current - previous)\n    in\n    seum\n      current\n      (accu + head)\n      (if prediction == tail then prediction else tail) (* validate prediction *)\n```\n\nThe program is executed with the same input and...\n\n```\n Performance counter stats for './a.out seum':\n\n            944,67 msec task-clock:u                     #    1,000 CPUs utilized\n     3 422 107 884      cycles:u                         #    3,623 GHz\n    16 006 647 403      instructions:u                   #    4,68  insn per cycle\n     5 001 230 197      branches:u                       #    5,294 G/sec\n           225 781      branch-misses:u                  #    0,00% of all branches\n```\n\nPer iteration:\n\n- 0.94 nanoseconds\n- 3.4 cycles\n- 16 instructions\n- 5 branches\n\nWe got past the bottleneck ! What we're effectively doing is transforming the list iteration into an array iteration. And this works, because there are situations where OCaml lists are allocated in a linear fashion, making the cell addresses predictible. Note in particular the huge number of instructions per cycle.\n\nIf you are not convinced, here is an updated dependency diagram for this program.\n\n![](./seum_chart.png)\n\nBy moving the tail pointer load outside of the critical path, the CPU is able to do more things at once. The following chart shows how one can expect the CPU to execute instructions for three iterations in parallel.\n\n![](./seum_chart_tiled.png)\n\n### The crux of the hack\n\nThe astute reader will wonder how is it possible to use `Obj.magic` to transmute the `int list` to an `int`. Indeed these OCaml types have different representations.\n- An `int list` is a pointer to an OCaml block allocated on the heap. Due to alignment, it always end with a zero bit.\n- An `int` is a primitive OCaml value, the value `n` being represented as the _tagged integer_ `2 * n + 1`.\n\nThis means that usual operations on `int` values won't work on raw pointers.\nFor example, the addition is implemented as `Int.add a b = a + b - 1`. The consequence is that adding two raw numbers together using the OCaml addition will subtract one from the result to account for the expected tags.\n\nFor subtraction, it is similar: `Int.sub a b = a - b + 1`.\n\nThe magic happens when we're combining an addition and a subtraction, so that the whole operation is correct on both pointers and integers:\n```ocaml\nInt.add current (Int.sub current previous)\n  = current + (current - previous + 1) - 1\n  = current + (current - previous)\n```\n\n<details>\n<summary>Here is the full annotated assembly for the curious</summary>\n```nasm\ncamlExample__seum_268:\n        subq    $8, %rsp\n.L103:\n        movq    %rax, %rsi\n        movq    %rdi, %rax\n        cmpq    (%r14), %r15           # check if GC is waiting for us\n        jbe     .L104                  # branch #1\n.L105:\n        testb   $1, %al                # check if at end of list\n        je      .L102                  # branch #2\n        movq    %rbx, %rax\n        addq    $8, %rsp\n        ret\n.L102:\n        movq    8(%rax), %rdx          # load tail element of list cell\n        movq    %rax, %rdi             # move cell pointer to %rdi\n        salq    $1, %rdi               # multiply pointer by two\n        subq    %rsi, %rdi             # subtract previous cell pointer to obtain the prediction\n        cmpq    %rdx, %rdi             # compare tail element with prediction\n        jne     .L101\n        jmp     .L100\n.L101:\n        movq    %rdx, %rdi             # prediction is incorrect, move the tail element in %rdi\n.L100:\n        movq    (%rax), %rsi           # load head element of list cell\n        leaq    -1(%rbx,%rsi), %rbx    # add to accumulator\n        jmp     .L103                  # loop\n.L104:\n        call    caml_call_gc@PLT\n.L106:\n        jmp     .L105\n```\n</details>\n\n### Optimized\n\nI was not satisfied by this mere 45% improvement. What if the loop was unrolled, so that the CPU has more freedom to move instructions around ?\n\n```ocaml\nlet rec seum_unroll (previous : int) (accu : int) (cell : int list) =\n  match cell with\n  | [] -> accu\n  | [a] ->  accu + a\n  | [a; b] -> accu + a + b\n  | [a; b; c] -> accu + a + b + c\n  | a::b::c::d::tail ->\n      let current = Obj.magic cell in\n      let prediction : int list =\n        Obj.magic (2 * current - previous)\n      in\n      seum_unroll\n        current\n        (accu + a + b + c + d)\n        (if prediction == tail then prediction else tail)\n```\n\nThe idea is to iterate on the list by chunks of 4 items. There is still the same amount of work to do, but the value prediction trick is performed once every 4 element.\n\n```\n Performance counter stats for './a.out seum_unroll':\n\n            715,58 msec task-clock:u                     #    0,999 CPUs utilized\n     2 410 830 333      cycles:u                         #    3,369 GHz\n     7 756 543 257      instructions:u                   #    3,22  insn per cycle\n     2 001 229 820      branches:u                       #    2,797 G/sec\n           112 956      branch-misses:u                  #    0,01% of all branches\n```\n\nPer iteration:\n\n- 0.72 nanoseconds\n- 2.4 cycles\n- 7.7 instructions\n- 2 branches\n\n<h1 style={{ fontSize: \"4em\", marginTop: \"-50px\", marginBottom: \"-50px\" }}>\n  <b>91.3%</b> faster. Nice.\n</h1>\n\nAccording the original article, we're now faster than the naive C implementation, but 2x slower than the hand-optimized one. This is all very synthetic but I found it quite interesting that OCaml is able to benefit from the same hacks as C. Of course this won't be useful in a lot of situations, but maybe one will gain some insights on how modern CPU are able to make programs go fast.\n\n---\n\nIf you like this article I'd be glad to have your feedback.\nPublic discussion on [Twitter](https://twitter.com/TheLortex/status/1654563811569836032), [Hacker News](https://news.ycombinator.com/item?id=35844078).\n"}},"pageContext":{"id":"f3c13877-2354-5007-93a8-e166b209bdf1","frontmatter__variant":"articles","frontmatter__slug":"value-speculation-ocaml","__params":{"frontmatter__variant":"articles","frontmatter__slug":"value-speculation-ocaml"},"frontmatter":{"title":"Implementing value speculation in OCaml","slug":"value-speculation-ocaml","date":"2023-05-05","category":"misc","description":"Value speculation exploits the CPU branch predictor to improve instruction parallelism. Here is an example of it in OCaml.","variant":"articles","hero_image":"./seum_hero.png","hero_image_alt":"This is convoluted"}}},"staticQueryHashes":["2744905544"],"slicesMap":{}}